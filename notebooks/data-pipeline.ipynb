{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Housekeeping",
   "id": "835956ee162ea320"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Library imports",
   "id": "1f0b532b2a1b6591"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if True:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install -r requirements.txt"
   ],
   "id": "ea8c7e0a1956487f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Core Libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from colorama import Fore, Style\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Machine Learning - Core\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "\n",
    "# Statistical & Data Processing\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Model Interpretability\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from supertree import SuperTree\n",
    "\n",
    "# Custom Functions\n",
    "from functions import (\n",
    "    random_forest_benchmark,\n",
    "    xgboost_benchmark,\n",
    "    adaboost_benchmark,\n",
    "    gradient_boosting_benchmark,\n",
    "    lightgbm_benchmark,\n",
    "    nn_feature_search,\n",
    "    explain_with_lime,\n",
    "    explain_with_shap,\n",
    "    cross_validate_feature_cutoffs,\n",
    "    plot_feature_cutoff_comparison,\n",
    "    plot_model_comparison_heatmap,\n",
    "    plot_residuals_analysis,\n",
    "    plot_prediction_intervals,\n",
    "    plot_learning_curves,\n",
    "    feature_selection_pipeline,\n",
    "    final_battle\n",
    ")"
   ],
   "id": "a6cdf2832f86683"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optional Imports (for specific analyses)\n",
    "\n",
    "These imports are used for specific visualizations and analyses that may not be needed in every run."
   ],
   "id": "3d3e1eb94964f207"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Optional: Advanced Visualizations\n",
    "# from sklearn.decomposition import PCA\n",
    "# from click.format import wrap_text  # May not be needed\n",
    "# import collections  # Built-in, no install needed\n",
    "\n",
    "# Note: Most visualization functions are now in functions.py\n",
    "# and imported above in the main imports cell"
   ],
   "id": "f9b973d464226a8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Settings",
   "id": "2ddb36ef38903cfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "print(Style.RESET_ALL)\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus or False: #<- Change to True if you want to torture your computer (:\n",
    "    RUN_NN = True\n",
    "    print(\"GPU found\")\n",
    "else:\n",
    "    RUN_NN = False"
   ],
   "id": "139473c427321a77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data imports\n",
    "Data was manually edited, to convert the mpa411.txt TSV format to a CSV format. Otherwise, Pandas was loading it as a single column, somehow. The first row, containing only \"#mpa_vJun23_CHOCOPhlAnSGB_202403\" was removed."
   ],
   "id": "e4c44185df7ba0b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = pd.read_csv('../data/raw/MAI3004_lucki_mpa411.csv')\n",
    "metadata = pd.read_csv('../data/raw/MAI3004_lucki_metadata_safe.csv')\n",
    "print(\n",
    "    f\"Data successfully imported. \\n shape of data: {data.shape} \\n \"\n",
    "    f\"Shape of metadata: {metadata.shape}\"\n",
    ")\n",
    "\n",
    "assert data.shape == (6903, 932), \"Data has the wrong shape. Check the CSV formatting.\"\n",
    "assert metadata.shape == (930, 6), \"Metadata has the wrong shape. Check the CSV formatting.\""
   ],
   "id": "2b85262b17d6ebe9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Function definitions\n",
    "| Function Name | Description | Parameters |\n",
    "|---------------|-------------|------------|\n"
   ],
   "id": "e66768567872eaa9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "8422ddff64f2a5a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge data and metadata",
   "id": "6f2ae59efaf10b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sample_cols = [col for col in data.columns if col.startswith(\"mpa411_\")]\n",
    "\n",
    "sample_abundances = (\n",
    "    data[['clade_name'] + sample_cols]\n",
    "    .set_index('clade_name')\n",
    "    .transpose()\n",
    "    .rename_axis('original_sample_id')\n",
    "    .reset_index()\n",
    "    .rename(columns={'original_sample_id': 'sample_id'})\n",
    ")\n",
    "\n",
    "sample_abundances[\"sample_id\"] = (\n",
    "    sample_abundances[\"sample_id\"].str.removeprefix(\n",
    "        \"mpa411_\",\n",
    "    )\n",
    ")\n",
    "\n",
    "metadata_common = metadata[\n",
    "    metadata[\"sample_id\"].isin(sample_abundances[\"sample_id\"])\n",
    "].copy()\n",
    "merged_samples = metadata_common.merge(\n",
    "    sample_abundances,\n",
    "    on=\"sample_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "merged_samples.drop(columns=['year_of_birth', 'body_product'], inplace=True)\n",
    "\n",
    "print(f\"Metadata rows (original): {metadata.shape[0]}\")\n",
    "print(f\"Metadata rows with matching samples: {metadata_common.shape[0]}\")\n",
    "print(\n",
    "    f\"Metadata rows without matching samples: \"\n",
    "    f\"{metadata_common.shape[0]-metadata_common.shape[0]}\"\n",
    ")\n",
    "print(f\"Merged dataframe shape: {merged_samples.shape}\")"
   ],
   "id": "3b83b9be727b6341"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "merged_samples.head()",
   "id": "7025be06a29c42fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoding",
   "id": "5a85f295ec5bb54d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Sex and family_ID\n",
    "encoded_samples = merged_samples.copy().dropna(subset=\"age_group_at_sample\")\n",
    "\n",
    "encoded_samples[\"sex\"] = (\n",
    "    encoded_samples[\"sex\"]\n",
    "    .fillna(\"unknown\")\n",
    "    .replace({\"female\": 1, \"male\": 0, \"unknown\": 2})\n",
    ")\n",
    "encoded_samples[\"family_id\"] = LabelEncoder().fit_transform(\n",
    "    encoded_samples[\"family_id\"]\n",
    ")\n"
   ],
   "id": "6d54e8e1d0c419e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Using days to better interpret the distance between age groups\n",
    "encoding_guide = {\n",
    "    '1-2 weeks': 10,\n",
    "    '4 weeks': 28,\n",
    "    '8 weeks': 56,\n",
    "    '4 months': 120,\n",
    "    '5 months': 150,\n",
    "    '6 months': 180,\n",
    "    '9 months': 270,\n",
    "    '11 months': 330,\n",
    "    '14 months': 420,\n",
    "}\n",
    "encoded_samples[\"age_group_at_sample\"] = encoded_samples[\"age_group_at_sample\"].replace(encoding_guide)\n",
    "encoded_samples[\"age_group_at_sample\"] = encoded_samples[\"age_group_at_sample\"].astype(int)\n",
    "# consider in interpretation that the distances between the real age bins are not the same as our age groups"
   ],
   "id": "a5be556402e40634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "leftovers = encoded_samples[encoded_samples[\"age_group_at_sample\"].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "if not leftovers.empty:\n",
    "\n",
    "    print(\"Age group encoding:\", leftovers[\"age_group_at_sample\"].unique())\n",
    "\n",
    "else:\n",
    "    print(\"Fallback encoding not needed\")\n"
   ],
   "id": "1f53df1bad616b8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missing check",
   "id": "6b54fa7ae03fe3ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "missing_table = (\n",
    "    encoded_samples.isna()\n",
    "    .sum()\n",
    "    .to_frame(name=\"missing_count\")\n",
    "    .assign(\n",
    "        missing_percent=lambda df: (\n",
    "            (df[\"missing_count\"] / encoded_samples.shape[0] * 100).round(2)\n",
    "        ),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"column\"})\n",
    "    .sort_values(\"missing_count\", ascending=False)\n",
    "    .query(\"missing_count != 0\")\n",
    ")\n",
    "\n",
    "if len(missing_table) > 0:\n",
    "    missing_table\n",
    "else:\n",
    "    print(\"No missing values detected.\")"
   ],
   "id": "263f5277d4a864c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Outlier check",
   "id": "331678dc31b3a291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "numeric_cols = encoded_samples.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "q1 = encoded_samples[numeric_cols].quantile(0.25)\n",
    "q3 = encoded_samples[numeric_cols].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "lower_bounds = q1 - 1.5 * iqr\n",
    "upper_bounds = q3 + 1.5 * iqr\n",
    "\n",
    "outlier_mask = (\n",
    "    (encoded_samples[numeric_cols] < lower_bounds)\n",
    "    | (encoded_samples[numeric_cols] > upper_bounds)\n",
    ")\n",
    "outlier_counts = outlier_mask.sum()\n",
    "outlier_percent = (outlier_counts / encoded_samples.shape[0] * 100).round(2)\n",
    "\n",
    "outlier_table = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": numeric_cols,\n",
    "        \"lower_bound\": lower_bounds,\n",
    "        \"upper_bound\": upper_bounds,\n",
    "        \"outlier_count\": outlier_counts,\n",
    "        \"outlier_percent\": outlier_percent,\n",
    "    })\n",
    "    .query(\"outlier_count > 0\")\n",
    "    .sort_values(\"outlier_percent\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "outlier_table"
   ],
   "id": "404b1ecb925633e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalisation check",
   "id": "774b607f6bc6a37b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "normalized_samples = encoded_samples.copy()\n",
    "print(\"Shapiro-Wilk Normality Test\")\n",
    "\n",
    "for column in numeric_cols:\n",
    "    data_nona = normalized_samples[column].dropna()\n",
    "    stat, p_value = stats.shapiro(data_nona)\n",
    "\n",
    "    if p_value > 0.05:\n",
    "        print(Fore.GREEN + f\"{column}: Normally Distributed (p={p_value:.4f})\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            Fore.RED\n",
    "            + f\"{column}: Not Normally Distributed (p={p_value:.4f})\"\n",
    "        )\n",
    "\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "44f69a39bd0adf0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train-test split before pre-processing",
   "id": "828b3b06ad9b5e45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "feature_cols = normalized_samples.columns.difference([\"sample_id\", \"age_group_at_sample\"]) # These variables will get removed from X\n",
    "\n",
    "X = normalized_samples[feature_cols]\n",
    "Y = normalized_samples[\"age_group_at_sample\"]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=3004)\n",
    "train_indicies, test_indicies = next(gss.split(X, Y, groups=X['family_id']))\n",
    "X_train_raw = X.iloc[train_indicies]\n",
    "X_test_raw = X.iloc[test_indicies]\n",
    "Y_train = Y.iloc[train_indicies]\n",
    "Y_test = Y.iloc[test_indicies]\n",
    "\n",
    "assert X_train_raw.shape[1] == X_test_raw.shape[1], \"Feature columns do not match between train and test sets.\"\n",
    "assert X_train_raw.shape[0] == Y_train.shape[0] and X_test_raw.shape[0] == Y_test.shape[0], \"X and Y do not have the same length.\"\n",
    "\n",
    "print(\"Train shape:\", X_train_raw.shape, \"| Test shape:\", X_test_raw.shape)"
   ],
   "id": "b9f3703894d6ad82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalising data using clr transformation",
   "id": "f7d47b92198950f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"The CLR function based on: https://medium.com/@nextgendatascientist/a-guide-for-data-scientists-log-ratio-transformations-in-machine-learning-a2db44e2a455\"\"\"\n",
    "\n",
    "def clr_transform(X, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute CLR with a tunable zero-replacement value (epsilon).\n",
    "    \"\"\"\n",
    "\n",
    "    #To capture metadata from the original dataframe\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        index = X.index\n",
    "        columns = X.columns\n",
    "        X_arr = X.values.astype(float)\n",
    "    else:\n",
    "        X_arr = np.array(X).astype(float)\n",
    "\n",
    "    # 1. Replace zeros with epsilon (tunable parameter)\n",
    "    X_replaced = np.where(X_arr == 0, epsilon, X_arr)\n",
    "\n",
    "    # 2. Compute Geometric Mean\n",
    "    # exp(mean(log)) is safer and standard for this\n",
    "    gm = np.exp(np.log(X_replaced).mean(axis=1, keepdims=True))\n",
    "\n",
    "    # 3. CLR transformation\n",
    "    X_clr = np.log(X_replaced / gm)\n",
    "\n",
    "\n",
    "    #Rebulding back a NumPy array to a dataframe\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return pd.DataFrame(X_clr, index=index, columns=columns)\n",
    "\n",
    "    return X_clr"
   ],
   "id": "abfed81b26ba1f6b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train = clr_transform(X_train_raw)\n",
    "X_test = clr_transform(X_test_raw)"
   ],
   "id": "ef29fe89048db549"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#To activate back the raw data without normalisation\n",
    "\n",
    "#X_train = X_train_raw\n",
    "#X_test = X_test_raw"
   ],
   "id": "a1b7c3769adc32ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Shapiro-Wilk Normality Test (after Log Normlaisation)\\n\")\n",
    "\n",
    "for col in X_train.columns:\n",
    "\n",
    "    # 1. Get the data for this column from both sets\n",
    "    train_data = X_train[col].dropna()\n",
    "    test_data = X_test[col].dropna()\n",
    "\n",
    "    # 2. Run Shapiro test on both\n",
    "    stat_train, p_train = stats.shapiro(train_data)\n",
    "    stat_test, p_test = stats.shapiro(test_data)\n",
    "\n",
    "    # 3. Determine status (Both must be > 0.05 to be truly \"Normal\")\n",
    "    is_train_normal = p_train > 0.05\n",
    "    is_test_normal = p_test > 0.05\n",
    "\n",
    "    # 4. Print Logic\n",
    "    # If both are Green\n",
    "    if is_train_normal and is_test_normal:\n",
    "        print(Fore.GREEN + f\"✔ {col}: Normal (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "    # If one is Red (Mixed results)\n",
    "    elif is_train_normal or is_test_normal:\n",
    "        print(Fore.YELLOW + f\"⚠ {col}: Inconsistent (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "    # If both are Red\n",
    "    else:\n",
    "        print(Fore.RED + f\"✘ {col}: Not Normal (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "f2ade2074676fe0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory data analysis",
   "id": "6ebaa601e6cf2970"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(merged_samples.shape)\n",
    "merged_samples.head()"
   ],
   "id": "333533f5f9d09029"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Dataset overview\n",
    "print(\"Number of samples:\", len(merged_samples))\n",
    "print(\n",
    "    \"Number of unique families (family_id):\",\n",
    "    merged_samples[\"family_id\"].nunique(),\n",
    ")\n",
    "print(\"Number of columns (metadata + features):\", merged_samples.shape[1])"
   ],
   "id": "d18cb3efdb854f5e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Samples per family\n",
    "samples_per_family = merged_samples[\"family_id\"].value_counts()\n",
    "samples_per_family.describe()"
   ],
   "id": "a5f7897a1360a5d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "samples_per_family.hist(bins=20)\n",
    "plt.xlabel(\"Number of samples per family\")\n",
    "plt.ylabel(\"Number of families\")\n",
    "plt.title(\"Distribution of samples per family\")\n",
    "plt.show()"
   ],
   "id": "cc7316e85d745203"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#distribution of age groups\n",
    "merged_samples[\"age_group_at_sample\"].value_counts(dropna=False)"
   ],
   "id": "bdcf76a9c476945f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_samples[\"age_group_at_sample\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Distribution of age groups\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "id": "4e3e29a554d48555"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#dimensionality and sparsity of the microbiome feature matrix\n",
    "metadata_cols = [\n",
    "    \"sample_id\",\n",
    "    \"family_id\",\n",
    "    \"sex\",\n",
    "    \"body_product\",\n",
    "    \"age_group_at_sample\",\n",
    "    \"year_of_birth\",\n",
    "]\n",
    "feature_cols = [c for c in merged_samples.columns if c not in metadata_cols]\n",
    "\n",
    "X = merged_samples[feature_cols]\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Overall fraction of zeros:\", (X == 0).mean().mean())"
   ],
   "id": "b2f9a7d34d3613ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#number of observed taxa per sample\n",
    "nonzero_per_sample = (X > 0).sum(axis=1)\n",
    "nonzero_per_sample.describe()"
   ],
   "id": "f2d18de3a2735d50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nonzero_per_sample.hist(bins=50)\n",
    "plt.xlabel(\"Number of non-zero taxa per sample\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Non-zero taxa per sample\")\n",
    "plt.show()"
   ],
   "id": "3ebc30d2bfef6b92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Total abundance per sample (sanity check)\n",
    "total_abundance = X.sum(axis=1)\n",
    "total_abundance.describe()"
   ],
   "id": "86e9c8a36505b371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "total_abundance.hist(bins=50)\n",
    "plt.xlabel(\"Total abundance per sample\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Total microbial abundance per sample\")\n",
    "plt.show()"
   ],
   "id": "530224c6e9c6721f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Distribution of feature prevalence\n",
    "feature_prevalence = (X > 0).sum(axis=0)\n",
    "feature_prevalence.describe()"
   ],
   "id": "5d90482f25b53635"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "feature_prevalence.hist(bins=50)\n",
    "plt.xlabel(\"Number of samples in which taxon is present\")\n",
    "plt.ylabel(\"Number of taxa\")\n",
    "plt.title(\"Feature prevalence distribution\")\n",
    "plt.show()"
   ],
   "id": "e76ce30593ba8a97"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Distribution of non-zero abundances (log scale)\n",
    "\n",
    "nonzero_values = X.values[X.values > 0]\n",
    "plt.hist(np.log10(nonzero_values), bins=50)\n",
    "plt.xlabel(\"log10(abundance)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of non-zero abundances (log10 scale)\")\n",
    "plt.show()"
   ],
   "id": "83132bff93298117"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# PCA visualization\n",
    "# Use a subset of features for speed\n",
    "prevalence = (X > 0).sum(axis=0)\n",
    "top_features = prevalence.sort_values(ascending=False).head(500).index\n",
    "\n",
    "X_sub = X[top_features]\n",
    "\n",
    "# Scale features\n",
    "X_scaled = StandardScaler().fit_transform(X_sub)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot\n",
    "age = merged_samples[\"age_group_at_sample\"]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca.iloc[:, 0], X_pca.iloc[:, 1],\n",
    "            c=pd.factorize(age)[0], cmap=\"viridis\", alpha=0.6)\n",
    "plt.colorbar(label=\"Age group (encoded)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA of samples colored by age group\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ],
   "id": "b004a9039cb6e6b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# PCA on CLR-transformed data (training set)\n",
    "\n",
    "# Use only microbiome features (exclude metadata)\n",
    "meta_cols = [c for c in [\"sex\", \"family_id\"] if c in X_train.columns]\n",
    "micro_cols = [c for c in X_train.columns if c not in meta_cols]\n",
    "\n",
    "Xtr = X_train[micro_cols].copy()\n",
    "Xte = X_test[micro_cols].copy()\n",
    "\n",
    "# Optional: select top N most prevalent taxa (for speed)\n",
    "top_n = 500\n",
    "if top_n is not None:\n",
    "    prevalence = (Xtr > 0).sum(axis=0)\n",
    "    top_features = prevalence.sort_values(ascending=False).head(top_n).index\n",
    "    Xtr = Xtr[top_features]\n",
    "    Xte = Xte[top_features]\n",
    "\n",
    "# Convert to NumPy\n",
    "Xtr_np = Xtr.to_numpy()\n",
    "Xte_np = Xte.to_numpy()\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "Xtr_scaled = scaler.fit_transform(Xtr_np)\n",
    "Xte_scaled = scaler.transform(Xte_np)\n",
    "\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2, random_state=3004)\n",
    "Ztr = pca.fit_transform(Xtr_scaled)\n",
    "Zte = pca.transform(Xte_scaled)\n",
    "\n",
    "# Force NumPy output again\n",
    "Ztr = np.asarray(Ztr)\n",
    "Zte = np.asarray(Zte)\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_, \"| Cumulative:\", pca.explained_variance_ratio_.sum())\n",
    "print(\"PC1 range:\", Ztr[:, 0].min(), Ztr[:, 0].max())\n",
    "print(\"PC2 range:\", Ztr[:, 1].min(), Ztr[:, 1].max())\n",
    "\n",
    "# Plot (high readability)\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "plt.figure(figsize=(8, 6), dpi=110)\n",
    "\n",
    "sc = plt.scatter(\n",
    "    Ztr[:, 0],\n",
    "    Ztr[:, 1],\n",
    "    c=Y_train.values,\n",
    "    s=35,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.85\n",
    ")\n",
    "\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"Age group (encoded)\", fontsize=12)\n",
    "cbar.ax.tick_params(labelsize=11)\n",
    "plt.xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}% variance explained)\", fontsize=13)\n",
    "plt.ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}% variance explained)\", fontsize=13)\n",
    "plt.title(\"PCA of CLR-transformed microbiome samples (training set)\", fontsize=15, pad=10)\n",
    "plt.xticks(fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(True, color=\"white\", alpha=0.3, linewidth=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "6d8d617718cddca9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary of EDA\n",
    "The dataset consists of 930 stool samples derived from multiple individuals across different families and contains approximately 6,900 microbiome features, making it a high-dimensional and highly sparse dataset. Each sample contains on average around 300 detected taxa, while the total microbial abundance per sample is relatively stable, indicating that sequencing depth is relatively consistent across samples.\n",
    "\n",
    "Most taxa are rare and occur in only a small fraction of samples, whereas a small subset of taxa is highly prevalent across the cohort. The distribution of non-zero abundances follows an approximately log-normal shape, which is typical for microbiome sequencing data (Lutz et al., 2022).\n",
    "\n",
    "An initial PCA projection based on the most prevalent taxa and raw abundance data does not reveal sharply separated clusters but shows a gradual age-related gradient, suggesting that age-related variation in microbiome composition is present but represents only a limited fraction of the total variance in the data.\n",
    "\n",
    "Because microbiome data are compositional in nature, a second PCA is later performed on CLR-transformed and standardized data in the preprocessing stage to obtain a compositionally valid low-dimensional representation for downstream analysis and modeling.\n"
   ],
   "id": "abcd463e6b342e12"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training",
   "id": "d5b370ca9e3f0472"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Filtering for features at the genus level",
   "id": "939df2bbee8c6bba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def filter_genus(df_uf): #Defining a function that filters a dataframe to only include columns with features at genus level\n",
    "    df_uf = df_uf.drop(list(df_uf.filter(regex=\"s__\")),axis=1,inplace=False) #Drops columns that include features at species level\n",
    "    df_uf = df_uf.filter(regex=\"g__\") #Drops columns that include features broader than genus level\n",
    "    return df_uf"
   ],
   "id": "e2476ffc73e89e61"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def filter_species(df_uf):\n",
    "    # Keep only columns with 's__' (species)\n",
    "    df_species = df_uf.filter(regex=\"s__\")\n",
    "\n",
    "    return df_species"
   ],
   "id": "ad162badb2109c47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Here you select whether X_train should contain species or genus\n",
    "\n",
    "X_train_genus = filter_genus(X_train)\n",
    "X_test_genus = filter_genus(X_test)"
   ],
   "id": "9bf3dd03d88dbea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "X_train_filtered, removed_features = feature_selection_pipeline(X_train_genus)\n",
    "X_test_filtered = X_test_genus[X_train_filtered.columns]\n",
    "\n",
    "X_train_genus = X_train_filtered.copy()\n",
    "X_test_genus = X_test_filtered.copy()"
   ],
   "id": "7378d3f602faf317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Features used by model:\")\n",
    "print(X_train_genus.columns.tolist())"
   ],
   "id": "7fadafee13c806d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest Regressor with Train/Test split (Genus)",
   "id": "63f90c37bca7b015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"# Default starting parameters\n",
    "current_params = {\n",
    "    \"prevalence_thresh\": 0.05,\n",
    "    \"abundance_thresh\": 1e-4,\n",
    "    \"variance_thresh\": 1e-5,\n",
    "    \"corr_thresh\": 0.9\n",
    "}\n",
    "\n",
    "# Maximum perturbation for each parameter\n",
    "param_ranges = {\n",
    "    \"prevalence_thresh\": 0.01,\n",
    "    \"abundance_thresh\": 5e-5,\n",
    "    \"variance_thresh\": 5e-6,\n",
    "    \"corr_thresh\": 0.05\n",
    "}\n",
    "\n",
    "best_rmse = np.inf\n",
    "best_r2 = -np.inf\n",
    "results = []\n",
    "\n",
    "n_iterations = 50  # you can increase this for a more thorough search\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    # Random perturbation for all parameters\n",
    "    new_params = {\n",
    "        k: max(1e-9, current_params[k] + np.random.uniform(-param_ranges[k], param_ranges[k]))\n",
    "        for k in current_params\n",
    "    }\n",
    "    # Keep corr_thresh <= 0.99\n",
    "    if new_params[\"corr_thresh\"] >= 1.0:\n",
    "        new_params[\"corr_thresh\"] = 0.99\n",
    "\n",
    "    # Apply feature filtering\n",
    "    X_train_filtered, removed_features = feature_selection_pipeline(\n",
    "        X_train_genus,\n",
    "        prevalence_thresh=new_params[\"prevalence_thresh\"],\n",
    "        abundance_thresh=new_params[\"abundance_thresh\"],\n",
    "        variance_thresh=new_params[\"variance_thresh\"],\n",
    "        corr_thresh=new_params[\"corr_thresh\"]\n",
    "    )\n",
    "    X_test_filtered = X_test_genus[X_train_filtered.columns]\n",
    "\n",
    "    # Train Gradient Boosting model\n",
    "    gb_results = gradient_boosting_benchmark(\n",
    "        X_train_filtered,\n",
    "        X_test_filtered,\n",
    "        Y_train,\n",
    "        Y_test,\n",
    "        label=f\"Genus Level Iter {i+1}\"\n",
    "    )\n",
    "\n",
    "    # Store result\n",
    "    results.append({\n",
    "        **new_params,\n",
    "        \"n_features\": X_train_filtered.shape[1],\n",
    "        \"rmse\": gb_results.rmse,\n",
    "        \"r2\": gb_results.r2\n",
    "    })\n",
    "\n",
    "    # Keep new params if performance improved\n",
    "    if gb_results.r2 > best_r2:  # maximizing R²\n",
    "        best_r2 = gb_results.r2\n",
    "        best_rmse = gb_results.rmse\n",
    "        current_params = new_params\n",
    "        print(f\"[Iter {i+1}] New best! RMSE: {best_rmse:.3f}, R2: {best_r2:.3f}, Params: {current_params}\")\n",
    "\n",
    "# Save all results for review\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSearch complete!\")\n",
    "print(f\"Best parameters: {current_params}\")\n",
    "print(f\"Best RMSE: {best_rmse:.3f}, Best R2: {best_r2:.3f}\")\n",
    "\"\"\""
   ],
   "id": "dfe7319fecfbc897"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Random Forest Regressor with Train/Test split (Genus)",
   "id": "4d3056e0c23db011"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Base model",
   "id": "a4fc0e2a49bc5536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Base model\n",
    "rf_base = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")\n",
    "\n",
    "rf_base.fit(X_train_genus, Y_train)\n",
    "yhat_rf = rf_base.predict(X_test_genus)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(Y_test, yhat_rf):.3f}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(mean_squared_error(Y_test, yhat_rf)):.3f}\")\n",
    "print(f\"R2 Score: {r2_score(Y_test, yhat_rf):.3f}\")\n"
   ],
   "id": "910c3dabeb83d3ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Search for the best model",
   "id": "ab9e72ca05aa8ec5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rf_results = random_forest_benchmark(\n",
    "    X_train_genus,\n",
    "    X_test_genus,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    label=\"Genus Level\"\n",
    ")\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {rf_results.best_params}\")"
   ],
   "id": "66eea4743b172296"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Mean Squared Error: {rf_results.rmse**2:.3f}\") # Squared because rmse is sqrt(mse)\n",
    "print(f\"Best CV RMSE: {rf_results.rmse:.3f}\")\n",
    "print(f\"R2 Score: {rf_results.r2:.3f}\")\n",
    "\n",
    "best_rf_model = rf_results.model\n",
    "yhat = rf_results.model.predict(X_test_genus)"
   ],
   "id": "aba2e256a16b4d9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=Y_test, y=yhat, alpha=0.6)\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs Actual (Genus-level RF)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9fc326e1cd456ee1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "residuals = Y_test - yhat\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=yhat, y=residuals, alpha=0.6)\n",
    "plt.axhline(0, linestyle=\"--\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predictions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "918377bf57e2bc94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "importances = pd.Series(\n",
    "    best_rf_model.feature_importances_,\n",
    "    index=X_train_genus.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    x=importances.head(top_n),\n",
    "    y=importances.head(top_n).index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Genus\")\n",
    "plt.title(f\"Top {top_n} most important genera\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d38b238324a29b2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "st = SuperTree(\n",
    "    best_rf_model,\n",
    "    X_train_genus,\n",
    "    Y_train\n",
    ")\n",
    "\n",
    "#st.show_tree(which_tree=0)"
   ],
   "id": "587ed86497c0b180"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# get predictions from each tree on the test set\n",
    "all_tree_preds = np.array([tree.predict(X_test_genus) for tree in best_rf_model.estimators_])\n",
    "\n",
    "# compute the mean prediction (Random Forest final prediction)\n",
    "rf_pred = all_tree_preds.mean(axis=0)\n",
    "\n",
    "# compute standard deviation per sample (uncertainty)\n",
    "rf_std = all_tree_preds.std(axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# plot all tree predictions (semi-transparent lines)\n",
    "for i in range(all_tree_preds.shape[0]):\n",
    "    plt.plot(Y_test.values, all_tree_preds[i], 'o', color='lightgray', alpha=0.3)\n",
    "\n",
    "# plot Random Forest mean prediction\n",
    "plt.scatter(Y_test, rf_pred, color='blue', label='RF mean prediction', s=40)\n",
    "\n",
    "plt.errorbar(Y_test, rf_pred, yerr=rf_std, fmt='o', color='red', alpha=0.5, label='±1 std across trees')\n",
    "\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color='black', linestyle='--', label='Perfect prediction')\n",
    "\n",
    "plt.xlabel(\"Actual Age Group\")\n",
    "plt.ylabel(\"Predicted Age Group\")\n",
    "plt.title(\"Random Forest – Forest plot of tree predictions\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "f631c6aa95a3506f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Alternative Models",
   "id": "3fade064b63458f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#For Software Developers! Delete this and keep only the lower Neural Network!\n",
    "\n",
    "if RUN_NN:\n",
    "    # For GPU\n",
    "    nn_res = nn_feature_search(\n",
    "        X_train, X_test, Y_train,\n",
    "        batch_size=8192,\n",
    "        device=\"/GPU:0\",\n",
    "        jit_compile=True,\n",
    "        mixed_precision=True\n",
    "    )\n",
    "\n",
    "    # For CPU\n",
    "    # nn_res = nn_feature_search(X_train, X_test, Y_train)\n",
    "\n",
    "    if nn_res:\n",
    "        elite_bacteria_list = nn_res.feature_names\n",
    "        print(f\"Selected {nn_res.n_features} elite bacterial drivers.\")\n",
    "        print(f\"Expected Validation RMSE: {nn_res.rmse:.2f} days\")\n",
    "    else:\n",
    "        print(\"No feature set found within the target range.\")"
   ],
   "id": "12dd809c62c0a46a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Also remove, just to check the XGboost of NN to the Genus level\n",
    "xgboost_results = xgboost_benchmark(\n",
    "    nn_res.X_train_elite,\n",
    "    nn_res.X_test_elite,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    label=\"Neural Network Level\"\n",
    ")\n",
    "print(f\"Best hyperparameters: {xgboost_results.best_params}\")\n",
    "## Additional Ensemble Methods"
   ],
   "id": "b5658d28299b186b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nn_res.X_train_elite.to_csv('X_train_NN_elite_raw.csv', index=True)\n",
    "nn_res.X_test_elite.to_csv('X_test_NN_elite_raw.csv', index=True)"
   ],
   "id": "24aaf9ca48fb36e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## XGBoost Alternative",
   "id": "bb0ccf4d84c792a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Base model\n",
    "xgb_base = XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "X_train_clean = X_train_genus.copy()\n",
    "X_train_clean.columns = [re.sub('[^A-Za-z0-9_]+', '', str(col)) for col in X_train_clean.columns]\n",
    "X_test_clean = X_test_genus.copy()\n",
    "X_test_clean.columns = [re.sub('[^A-Za-z0-9_]+', '', str(col)) for col in X_test_clean.columns]\n",
    "\n",
    "\n",
    "xgb_base.fit(X_train_clean, Y_train)\n",
    "yhat_xgb = xgb_base.predict(X_test_clean)\n",
    "\n",
    "print(f\"Mean Squared Error: {mean_squared_error(Y_test, yhat_xgb):.3f}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(mean_squared_error(Y_test, yhat_xgb)):.3f}\")\n",
    "print(f\"R2 Score: {r2_score(Y_test, yhat_xgb):.3f}\")\n",
    "\n",
    "#Best hyperparameters: {'subsample': 0.7, 'reg_lambda': 1.0, 'reg_alpha': 0.1, 'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.2}"
   ],
   "id": "b72788954866c93f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Best XGBoost Parameters Search",
   "id": "ced218eda77ae25a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xgboost_results = xgboost_benchmark(\n",
    "    X_train_genus,\n",
    "    X_test_genus,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    label=\"Genus Level\"\n",
    ")\n",
    "print(f\"Best hyperparameters: {xgboost_results.best_params}\")"
   ],
   "id": "9f020122629a75fb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Additional Ensemble Methods",
   "id": "3243205875e05420"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### AdaBoost",
   "id": "f79e4cf7061ead95"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "adaboost_results = adaboost_benchmark(\n",
    "    X_train_genus,\n",
    "    X_test_genus,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    label=\"Genus Level\"\n",
    ")\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {adaboost_results.best_params}\")"
   ],
   "id": "732cdaad40477a4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Mean Squared Error: {adaboost_results.rmse**2:.3f}\")\n",
    "print(f\"Best CV RMSE: {adaboost_results.rmse:.3f}\")\n",
    "print(f\"R2 Score: {adaboost_results.r2:.3f}\")\n",
    "\n",
    "best_adaboost_model = adaboost_results.model\n",
    "yhat_ada = adaboost_results.model.predict(X_test_genus)"
   ],
   "id": "fc1dc3aef9b7b4c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=Y_test, y=yhat_ada, alpha=0.6)\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs Actual (Genus-level AdaBoost)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "97b2616826d8f023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "importances_ada = pd.Series(\n",
    "    best_adaboost_model.feature_importances_,\n",
    "    index=X_train_genus.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    x=importances_ada.head(top_n),\n",
    "    y=importances_ada.head(top_n).index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Genus\")\n",
    "plt.title(f\"Top {top_n} most important genera (AdaBoost)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3d80caf60c83ed11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Gradient Boosting",
   "id": "205f6bae0fd68ebe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gb_results = gradient_boosting_benchmark(\n",
    "    X_train_genus,\n",
    "    X_test_genus,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    label=\"Genus Level\"\n",
    ")\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {gb_results.best_params}\")"
   ],
   "id": "70650f8946d28a71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Mean Squared Error: {gb_results.rmse**2:.3f}\")\n",
    "print(f\"Best CV RMSE: {gb_results.rmse:.3f}\")\n",
    "print(f\"R2 Score: {gb_results.r2:.3f}\")\n",
    "\n",
    "best_gb_model = gb_results.model\n",
    "yhat_gb = gb_results.model.predict(X_test_genus)"
   ],
   "id": "c1737693976b0d88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=Y_test, y=yhat_gb, alpha=0.6)\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs Actual (Genus-level Gradient Boosting)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3d4694eaaea8bdd1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "importances_gb = pd.Series(\n",
    "    best_gb_model.feature_importances_,\n",
    "    index=X_train_genus.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    x=importances_gb.head(top_n),\n",
    "    y=importances_gb.head(top_n).index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Genus\")\n",
    "plt.title(f\"Top {top_n} most important genera (Gradient Boosting)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "64223c5eadd12910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LightGBM",
   "id": "c3313c6901b5a261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lgb_results = lightgbm_benchmark(\n",
    "    X_train_genus,\n",
    "    X_test_genus,\n",
    "    Y_train,\n",
    "    Y_test,\n",
    "    label=\"Genus Level\"\n",
    ")\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {lgb_results.best_params}\")"
   ],
   "id": "c3e039571de6bd8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Mean Squared Error: {lgb_results.rmse**2:.3f}\")\n",
    "print(f\"Best CV RMSE: {lgb_results.rmse:.3f}\")\n",
    "print(f\"R2 Score: {lgb_results.r2:.3f}\")\n",
    "\n",
    "best_lgb_model = lgb_results.model\n",
    "yhat_lgb = lgb_results.model.predict(X_test_genus)"
   ],
   "id": "6327466c76b7f433"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=Y_test, y=yhat_lgb, alpha=0.6)\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs Actual (Genus-level LightGBM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6cb1caffe2d500c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "importances_lgb = pd.Series(\n",
    "    best_lgb_model.feature_importances_,\n",
    "    index=X_train_genus.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    x=importances_lgb.head(top_n),\n",
    "    y=importances_lgb.head(top_n).index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Genus\")\n",
    "plt.title(f\"Top {top_n} most important genera (LightGBM)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1dbc32b99724ea37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Interpretability with LIME and SHAP",
   "id": "fdad439946dbfdcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LIME (Local Interpretable Model-agnostic Explanations)",
   "id": "65ff9d3183ffc181"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the Random Forest model for explanation\n",
    "lime_explanations = explain_with_lime(\n",
    "    model=best_rf_model,\n",
    "    X_train=X_train_genus,\n",
    "    X_test=X_test_genus,\n",
    "    feature_names=X_train_genus.columns.tolist(),\n",
    "    num_samples=3,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "print(\"LIME Explanations for Random Forest Model:\")\n",
    "print(f\"Number of samples explained: {len(lime_explanations)}\")"
   ],
   "id": "3becae617b8cbd94"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lime_explanations = explain_with_lime(\n",
    "    model=best_rf_model,\n",
    "    X_train=X_train_genus,\n",
    "    X_test=X_test_genus,\n",
    "    feature_names=X_train_genus.columns.tolist(),\n",
    "    num_samples=3,\n",
    "    num_features=10\n",
    ")\n",
    "\n",
    "y_test_np = Y_test.values if hasattr(Y_test, \"values\") else Y_test\n",
    "\n",
    "for i, (sample_key, explanation) in enumerate(lime_explanations.items()):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sample {sample_idx}:\")\n",
    "    print(f\"Predicted value: {explanation['predicted_value']:.3f}\")\n",
    "    print(f\"Actual value: {explanation['actual_value']:.3f}\")\n",
    "    print(f\"\\nTop contributing features:\")\n",
    "    for feature, weight in explanation['explanation'][:10]:\n",
    "        print(f\"  {feature}: {weight:+.4f}\")"
   ],
   "id": "3d466fd0aa003095"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### SHAP (SHapley Additive exPlanations)",
   "id": "601e3a65966af8ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the Random Forest model for SHAP explanation\n",
    "shap_result = explain_with_shap(\n",
    "    model=best_rf_model,\n",
    "    X_train=X_train_genus,\n",
    "    X_test=X_test_genus,\n",
    "    feature_names=X_train_genus.columns.tolist(),\n",
    "    background_samples=100\n",
    ")\n",
    "\n",
    "print(\"SHAP Analysis Complete\")\n",
    "print(f\"SHAP values shape: {shap_result['shap_values'].shape}\")"
   ],
   "id": "34683d736c55be65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import shap\n",
    "\n",
    "# Summary plot showing feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_result['shap_values'],\n",
    "    X_test_genus,\n",
    "    feature_names=X_train_genus.columns.tolist(),\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "661f0bdbdffeb808"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Bar plot of mean absolute SHAP values\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(\n",
    "    shap_result['shap_values'],\n",
    "    X_test_genus,\n",
    "    feature_names=X_train_genus.columns.tolist(),\n",
    "    plot_type=\"bar\",\n",
    "    max_display=20,\n",
    "    show=False\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "30585140de63d5f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Selection via neural networks",
   "id": "359efcac37a66e0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The finding",
   "id": "23d8d9cae2a4c9c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if RUN_NN:\n",
    "    # For GPU\n",
    "    #nn_res = nn_feature_search(\n",
    "    #    X_train, X_test, Y_train,\n",
    "    #    batch_size=8192,\n",
    "    #    device=\"/GPU:0\",\n",
    "    #    jit_compile=True,\n",
    "    #    mixed_precision=True)\n",
    "\n",
    "    # For CPU\n",
    "    nn_res = nn_feature_search(X_train, X_test, Y_train)\n",
    "\n",
    "    if nn_res:\n",
    "        elite_bacteria_list = nn_res.feature_names\n",
    "        print(f\"Selected {nn_res.n_features} elite bacterial drivers.\")\n",
    "        print(f\"Expected Validation RMSE: {nn_res.rmse:.2f} days\")\n",
    "    else:\n",
    "        print(\"No feature set found within the target range.\")\n",
    "## XGBoost Alternative"
   ],
   "id": "cf389eded21c2d11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing the genus vs neural network features on all models",
   "id": "575cebdf5cde7c8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Feature Cutoff Cross-Validation\n",
    "\n",
    "Test model performance at different taxonomic levels"
   ],
   "id": "167d1ae9b1417ea8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test Random Forest at different taxonomic levels\n",
    "rf_cutoff_results = cross_validate_feature_cutoffs(\n",
    "    X_train_raw,\n",
    "    Y_train,\n",
    "    feature_levels=['Phylum', 'Class', 'Order', 'Family', 'Genus', 'Species'],\n",
    "    model_type='RandomForest',\n",
    "    cv_folds=5\n",
    ")"
   ],
   "id": "ea1e720e80a783c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize the results\n",
    "plot_feature_cutoff_comparison(rf_cutoff_results, title=\"Random Forest Performance\")"
   ],
   "id": "d9ce2c3dae2372bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compare Different Models Across Taxonomic Levels",
   "id": "8754619385db706c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Test other models\n",
    "xgb_cutoff_results = cross_validate_feature_cutoffs(\n",
    "    X_train_raw,\n",
    "    Y_train,\n",
    "    feature_levels=['Phylum', 'Class', 'Order', 'Family', 'Genus'],\n",
    "    model_type='XGBoost',\n",
    "    cv_folds=5\n",
    ")"
   ],
   "id": "2f8d8fca5d9c838a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "lgbm_cutoff_results = cross_validate_feature_cutoffs(\n",
    "    X_train_raw,\n",
    "    Y_train,\n",
    "    feature_levels=['Phylum', 'Class', 'Order', 'Family', 'Genus'],\n",
    "    model_type='LightGBM',\n",
    "    cv_folds=5\n",
    ")"
   ],
   "id": "a9731c7c2d800e74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Advanced Model Visualizations",
   "id": "7ba15ca6203efa09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Residuals analysis for best Random Forest model\n",
    "plot_residuals_analysis(Y_test, yhat, title=\"Random Forest Residuals Analysis\")"
   ],
   "id": "ef434196d363a7ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Prediction intervals (showing uncertainty from individual trees)\n",
    "plot_prediction_intervals(\n",
    "    Y_test, \n",
    "    yhat,\n",
    "    prediction_std=rf_std,\n",
    "    sample_indices=range(50),  # Show first 50 samples\n",
    "    title=\"Random Forest Predictions with Uncertainty\"\n",
    ")"
   ],
   "id": "e11bda9aade7aa9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Learning curves to check for overfitting\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "plot_learning_curves(\n",
    "    rf_model,\n",
    "    X_train_genus,\n",
    "    Y_train,\n",
    "    cv_folds=5,\n",
    "    title=\"Random Forest Learning Curves (Genus Level)\"\n",
    ")"
   ],
   "id": "3ca2f1f3513273b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Comparison",
   "id": "bb9c349136f6772e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compare all models (assuming they've been run)\n",
    "try:\n",
    "    model_results = {\n",
    "        'Random Forest': rf_results,\n",
    "        'AdaBoost': adaboost_results,\n",
    "        'Gradient Boosting': gb_results,\n",
    "        'LightGBM': lgb_results\n",
    "    }\n",
    "    plot_model_comparison_heatmap(model_results, title=\"Model Performance Comparison\")\n",
    "except NameError:\n",
    "    print(\"Some models haven't been run yet. Run all benchmark functions first.\")"
   ],
   "id": "90dfd437356f3860"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "try:\n",
    "    nn_res\n",
    "except NameError:\n",
    "    print(\"Neural network did not run\")\n",
    "else:\n",
    "        feature_sets = {\n",
    "        \"Genus Level\": X_train_genus,\n",
    "        \"NN Level\": nn_res.X_train_elite\n",
    "    }\n",
    "        battle_stats = final_battle(feature_sets, Y_train)"
   ],
   "id": "dbbc06dac82509e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
