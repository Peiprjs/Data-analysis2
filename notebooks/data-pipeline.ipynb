{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Housekeeping",
   "id": "245a8e452ba91453"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Library imports",
   "id": "b20a48091de21310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if True:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install -r requirements.txt"
   ],
   "id": "6a35b206f820bd0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: remove unnecessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from click.formatting import iter_rows\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RepeatedStratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from supertree import SuperTree"
   ],
   "id": "267b4a12030b559b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Settings",
   "id": "22f6afe07dae0590"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "f6f6ef7d9493cd17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data imports\n",
    "Data was manually edited, to convert the mpa411.txt TSV format to a CSV format. Otherwise, Pandas was loading it as a single column, somehow. The first row, containing only \"#mpa_vJun23_CHOCOPhlAnSGB_202403\" was removed."
   ],
   "id": "a9398ff41d8ed543"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../data/raw/MAI3004_lucki_mpa411.csv')\n",
    "metadata = pd.read_csv('../data/raw/MAI3004_lucki_metadata_safe.csv')\n",
    "print(\n",
    "    f\"Data successfully imported. \\n shape of data: {data.shape} \\n \"\n",
    "    f\"Shape of metadata: {metadata.shape}\"\n",
    ")\n",
    "\n",
    "assert data.shape == (6903, 932), \"Data has the wrong shape. Check the CSV formatting.\"\n",
    "assert metadata.shape == (930, 6), \"Metadata has the wrong shape. Check the CSV formatting.\"\n"
   ],
   "id": "890e64ba932e7f65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Function definitions\n",
    "| Function Name | Description | Parameters |\n",
    "|---------------|-------------|------------|\n"
   ],
   "id": "9d21d71d1c214e8e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "5e9f672bf329ed95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge data and metadata",
   "id": "126792a1db6fb384"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_cols = [col for col in data.columns if col.startswith(\"mpa411_\")]\n",
    "\n",
    "sample_abundances = (\n",
    "    data[['clade_name'] + sample_cols]\n",
    "    .set_index('clade_name')\n",
    "    .transpose()\n",
    "    .rename_axis('original_sample_id')\n",
    "    .reset_index()\n",
    "    .rename(columns={'original_sample_id': 'sample_id'})\n",
    ")\n",
    "\n",
    "sample_abundances[\"sample_id\"] = (\n",
    "    sample_abundances[\"sample_id\"].str.removeprefix(\n",
    "        \"mpa411_\",\n",
    "    )\n",
    ")\n",
    "\n",
    "metadata_common = metadata[\n",
    "    metadata[\"sample_id\"].isin(sample_abundances[\"sample_id\"])\n",
    "].copy()\n",
    "merged_samples = metadata_common.merge(\n",
    "    sample_abundances,\n",
    "    on=\"sample_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "merged_samples.drop(columns=['year_of_birth', 'body_product'], inplace=True)\n",
    "# YOB and body_product are omitted without sample dates.\n",
    "# All samples are fecal.\n",
    "# TODO: should we be accounting for sex? Do statistical analysis\n",
    "\n",
    "print(f\"Metadata rows (original): {metadata.shape[0]}\")\n",
    "print(f\"Metadata rows with matching samples: {metadata_common.shape[0]}\")\n",
    "print(\n",
    "    f\"Metadata rows without matching samples: \"\n",
    "    f\"{metadata_common.shape[0]-metadata_common.shape[0]}\"\n",
    ")\n",
    "print(f\"Merged dataframe shape: {merged_samples.shape}\")"
   ],
   "id": "cb744291054dc1d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_samples.head()",
   "id": "8ae00915d8eb3714",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoding",
   "id": "226450d88bcae44c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sex and family_ID\n",
    "encoded_samples = merged_samples.copy().dropna(subset=\"age_group_at_sample\")\n",
    "\n",
    "encoded_samples[\"sex\"] = (\n",
    "    encoded_samples[\"sex\"]\n",
    "    .fillna(\"unknown\")\n",
    "    .replace({\"female\": 1, \"male\": 0, \"unknown\": 2})\n",
    ")\n",
    "encoded_samples[\"family_id\"] = LabelEncoder().fit_transform(\n",
    "    encoded_samples[\"family_id\"]\n",
    ")\n"
   ],
   "id": "f0b746a5253a72f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Using days to better interpret the distance between age groups\n",
    "encoding_guide = {\n",
    "    '1-2 weeks': 10,\n",
    "    '4 weeks': 28,\n",
    "    '8 weeks': 56,\n",
    "    '4 months': 120,\n",
    "    '5 months': 150,\n",
    "    '6 months': 180,\n",
    "    '9 months': 270,\n",
    "    '11 months': 330,\n",
    "    '14 months': 420,\n",
    "}\n",
    "encoded_samples[\"age_group_at_sample\"].replace(encoding_guide, inplace=True)\n",
    "\n",
    "# consider in interpretation that the distances between the real age bins are not the same as our age groups"
   ],
   "id": "aa6e8781f0d48498",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False in pd.DataFrame(encoded_samples[\"age_group_at_sample\"]).applymap(np.isreal): #fallback encoder\n",
    "    age_encoder = LabelEncoder().fit(encoded_samples[\"age_group_at_sample\"])\n",
    "    encoded_samples[\"age_group_at_sample\"] = age_encoder.transform(\n",
    "        encoded_samples[\"age_group_at_sample\"]\n",
    "    )\n",
    "\n",
    "    age_groups = dict(\n",
    "        zip(age_encoder.classes_, age_encoder.transform(age_encoder.classes_))\n",
    "    )\n",
    "    print(\"Age group encoding:\", age_groups)\n",
    "\n",
    "else:\n",
    "    print(\"Fallback encoding not needed\")"
   ],
   "id": "88e9447f0a7d066f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missing check",
   "id": "ecd816ad7b428824"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_table = (\n",
    "    encoded_samples.isna()\n",
    "    .sum()\n",
    "    .to_frame(name=\"missing_count\")\n",
    "    .assign(\n",
    "        missing_percent=lambda df: (\n",
    "            (df[\"missing_count\"] / encoded_samples.shape[0] * 100).round(2)\n",
    "        ),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"column\"})\n",
    "    .sort_values(\"missing_count\", ascending=False)\n",
    "    .query(\"missing_count != 0\")\n",
    ")\n",
    "\n",
    "if len(missing_table) > 0:\n",
    "    missing_table\n",
    "else:\n",
    "    print(\"No missing values detected.\")"
   ],
   "id": "626e444ee2e1fb9d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Outlier check",
   "id": "bbdfec74a7c54fb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_cols = encoded_samples.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "q1 = encoded_samples[numeric_cols].quantile(0.25)\n",
    "q3 = encoded_samples[numeric_cols].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "lower_bounds = q1 - 1.5 * iqr\n",
    "upper_bounds = q3 + 1.5 * iqr\n",
    "\n",
    "outlier_mask = (\n",
    "    (encoded_samples[numeric_cols] < lower_bounds)\n",
    "    | (encoded_samples[numeric_cols] > upper_bounds)\n",
    ")\n",
    "outlier_counts = outlier_mask.sum()\n",
    "outlier_percent = (outlier_counts / encoded_samples.shape[0] * 100).round(2)\n",
    "\n",
    "outlier_table = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": numeric_cols,\n",
    "        \"lower_bound\": lower_bounds,\n",
    "        \"upper_bound\": upper_bounds,\n",
    "        \"outlier_count\": outlier_counts,\n",
    "        \"outlier_percent\": outlier_percent,\n",
    "    })\n",
    "    .query(\"outlier_count > 0\")\n",
    "    .sort_values(\"outlier_percent\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "outlier_table"
   ],
   "id": "32aa2bc69f3bd02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalisation check",
   "id": "c9a10115d763bdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normalized_samples = encoded_samples.copy()\n",
    "print(\"Shapiro-Wilk Normality Test\")\n",
    "\n",
    "for column in numeric_cols:\n",
    "    data_nona = normalized_samples[column].dropna()\n",
    "    stat, p_value = stats.shapiro(data_nona)\n",
    "\n",
    "    if p_value > 0.05:\n",
    "        print(Fore.GREEN + f\"{column}: Normally Distributed (p={p_value:.4f})\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            Fore.RED\n",
    "            + f\"{column}: Not Normally Distributed (p={p_value:.4f})\"\n",
    "        )\n",
    "\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "ee40e5c046a69e52",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train-test split before pre-processing",
   "id": "3619d97a8599c6ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "feature_cols = normalized_samples.columns.difference([\"sample_id\", \"age_group_at_sample\"]) # These variables will get removed from X\n",
    "\n",
    "X = normalized_samples[feature_cols]\n",
    "Y = normalized_samples[\"age_group_at_sample\"]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=3004)\n",
    "train_indicies, test_indicies = next(gss.split(X, Y, groups=X['family_id']))\n",
    "X_train_raw = X.iloc[train_indicies]\n",
    "X_test_raw = X.iloc[test_indicies]\n",
    "Y_train = Y.iloc[train_indicies]\n",
    "Y_test = Y.iloc[test_indicies]\n",
    "\n",
    "assert X_train_raw.shape[1] == X_test_raw.shape[1], \"Feature columns do not match between train and test sets.\"\n",
    "assert X_train_raw.shape[0] == Y_train.shape[0] and X_test_raw.shape[0] == Y_test.shape[0], \"X and Y do not have the same length.\"\n",
    "\n",
    "print(\"Train shape:\", X_train_raw.shape, \"| Test shape:\", X_test_raw.shape)"
   ],
   "id": "80e1fc5192fa796b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalising data using clr transformation",
   "id": "64f8805ca3d312a6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"The CLR function based on: https://medium.com/@nextgendatascientist/a-guide-for-data-scientists-log-ratio-transformations-in-machine-learning-a2db44e2a455\"\"\"\n",
    "\n",
    "def clr_transform(X, epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Compute CLR with a tunable zero-replacement value (epsilon).\n",
    "    \"\"\"\n",
    "\n",
    "    #To capture metadata from the original dataframe\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        index = X.index\n",
    "        columns = X.columns\n",
    "        X_arr = X.values\n",
    "    else:\n",
    "        X_arr = X\n",
    "\n",
    "    # 1. Replace zeros with epsilon (tunable parameter)\n",
    "    X_replaced = np.where(X == 0, epsilon, X)\n",
    "\n",
    "    # 2. Compute Geometric Mean\n",
    "    # exp(mean(log)) is safer and standard for this\n",
    "    gm = np.exp(np.log(X_replaced).mean(axis=1, keepdims=True))\n",
    "\n",
    "    # 3. CLR transformation\n",
    "    X_clr = np.log(X_replaced / gm)\n",
    "\n",
    "\n",
    "    #Rebulding back a NumPy array to a dataframe\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        return pd.DataFrame(X_clr, index=index, columns=columns)\n",
    "\n",
    "    return X_clr"
   ],
   "id": "84265063914f513a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = clr_transform(X_train_raw)\n",
    "X_test = clr_transform(X_test_raw)"
   ],
   "id": "193f715fbca3525a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#To activate back the raw data without normalisation\n",
    "\n",
    "#X_train = X_train_raw\n",
    "#X_test = X_test_raw"
   ],
   "id": "3b484009d29a58d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Shapiro-Wilk Normality Test (after Log Normlaisation)\\n\")\n",
    "\n",
    "for col in X_train.columns:\n",
    "\n",
    "    # 1. Get the data for this column from both sets\n",
    "    train_data = X_train[col].dropna()\n",
    "    test_data = X_test[col].dropna()\n",
    "\n",
    "    # 2. Run Shapiro test on both\n",
    "    stat_train, p_train = stats.shapiro(train_data)\n",
    "    stat_test, p_test = stats.shapiro(test_data)\n",
    "\n",
    "    # 3. Determine status (Both must be > 0.05 to be truly \"Normal\")\n",
    "    is_train_normal = p_train > 0.05\n",
    "    is_test_normal = p_test > 0.05\n",
    "\n",
    "    # 4. Print Logic\n",
    "    # If both are Green\n",
    "    if is_train_normal and is_test_normal:\n",
    "        print(Fore.GREEN + f\"‚úî {col}: Normal (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "    # If one is Red (Mixed results)\n",
    "    elif is_train_normal or is_test_normal:\n",
    "        print(Fore.YELLOW + f\"‚ö† {col}: Inconsistent (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "    # If both are Red\n",
    "    else:\n",
    "        print(Fore.RED + f\"‚úò {col}: Not Normal (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "b7ed25037599f315",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory data analysis",
   "id": "ec8214ce5862d929"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(merged_samples.shape)\n",
    "merged_samples.head()"
   ],
   "id": "7155f5c71e429871",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset overview\n",
    "print(\"Number of samples:\", len(merged_samples))\n",
    "print(\n",
    "    \"Number of unique families (family_id):\",\n",
    "    merged_samples[\"family_id\"].nunique(),\n",
    ")\n",
    "print(\"Number of columns (metadata + features):\", merged_samples.shape[1])"
   ],
   "id": "3d80e74f7efff9f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Samples per family\n",
    "samples_per_family = merged_samples[\"family_id\"].value_counts()\n",
    "samples_per_family.describe()"
   ],
   "id": "2bc5e5d2f5f063fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "samples_per_family.hist(bins=20)\n",
    "plt.xlabel(\"Number of samples per family\")\n",
    "plt.ylabel(\"Number of families\")\n",
    "plt.title(\"Distribution of samples per family\")\n",
    "plt.show()"
   ],
   "id": "dc1423022712d7ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#distribution of age groups\n",
    "merged_samples[\"age_group_at_sample\"].value_counts(dropna=False)"
   ],
   "id": "c230c5e3ee6a406a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged_samples[\"age_group_at_sample\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Distribution of age groups\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "id": "991cc1e66db3db86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#dimensionality and sparsity of the microbiome feature matrix\n",
    "metadata_cols = [\n",
    "    \"sample_id\",\n",
    "    \"family_id\",\n",
    "    \"sex\",\n",
    "    \"body_product\",\n",
    "    \"age_group_at_sample\",\n",
    "    \"year_of_birth\",\n",
    "]\n",
    "feature_cols = [c for c in merged_samples.columns if c not in metadata_cols]\n",
    "\n",
    "X = merged_samples[feature_cols]\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Overall fraction of zeros:\", (X == 0).mean().mean())"
   ],
   "id": "4e8ac4dc9ae13c10",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#number of observed taxa per sample\n",
    "nonzero_per_sample = (X > 0).sum(axis=1)\n",
    "nonzero_per_sample.describe()"
   ],
   "id": "9cfccc0be8ba7125",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nonzero_per_sample.hist(bins=50)\n",
    "plt.xlabel(\"Number of non-zero taxa per sample\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Non-zero taxa per sample\")\n",
    "plt.show()"
   ],
   "id": "5ca8d17368c32eea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Total abundance per sample (sanity check)\n",
    "total_abundance = X.sum(axis=1)\n",
    "total_abundance.describe()"
   ],
   "id": "813d8e69f9ef8a2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_abundance.hist(bins=50)\n",
    "plt.xlabel(\"Total abundance per sample\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Total microbial abundance per sample\")\n",
    "plt.show()"
   ],
   "id": "36d47d0499c2ac0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of feature prevalence\n",
    "feature_prevalence = (X > 0).sum(axis=0)\n",
    "feature_prevalence.describe()"
   ],
   "id": "783cb2d28a53d27f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feature_prevalence.hist(bins=50)\n",
    "plt.xlabel(\"Number of samples in which taxon is present\")\n",
    "plt.ylabel(\"Number of taxa\")\n",
    "plt.title(\"Feature prevalence distribution\")\n",
    "plt.show()"
   ],
   "id": "5281aa43e4562459",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of non-zero abundances (log scale)\n",
    "\n",
    "nonzero_values = X.values[X.values > 0]\n",
    "plt.hist(np.log10(nonzero_values), bins=50)\n",
    "plt.xlabel(\"log10(abundance)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of non-zero abundances (log10 scale)\")\n",
    "plt.show()"
   ],
   "id": "db926589f449cd18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PCA visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use a subset of features for speed\n",
    "prevalence = (X > 0).sum(axis=0)\n",
    "top_features = prevalence.sort_values(ascending=False).head(500).index\n",
    "\n",
    "X_sub = X[top_features]\n",
    "\n",
    "# Scale features\n",
    "X_scaled = StandardScaler().fit_transform(X_sub)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot\n",
    "age = merged_samples[\"age_group_at_sample\"]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca.iloc[:, 0], X_pca.iloc[:, 1],\n",
    "            c=pd.factorize(age)[0], cmap=\"viridis\", alpha=0.6)\n",
    "plt.colorbar(label=\"Age group (encoded)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA of samples colored by age group\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ],
   "id": "54f02d274a912fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary of EDA\n",
    "The dataset consists of 930 stool samples derived from multiple individuals across different families and contains approximately 6,900 microbiome features, making it a high-dimensional and highly sparse dataset. Each sample contains on average around 300 detected taxa, while the total microbial abundance per sample is relatively stable, indicating that sequencing depth is consistent across samples.\n",
    "Most taxa are rare and occur in only a small fraction of samples, whereas a small subset of taxa is highly prevalent across the cohort. The distribution of non-zero abundances follows an approximately log-normal shape, which is typical for microbiome sequencing data (e.g., Lutz et al., 2022).\n",
    "A PCA projection based on the most prevalent taxa does not reveal sharply separated clusters but shows a gradual age-related gradient, suggesting that age-related variation in microbiome composition is present but represents only a limited fraction of the total variance in the data."
   ],
   "id": "f7530865cf36e986"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training",
   "id": "8cbfbf00358f394f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtering for features at the genus level",
   "id": "a2e2424b2ba2f42e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def filter_genus(df_uf): #Defining a function that filters a dataframe to only include columns with features at genus level\n",
    "    df_uf = df_uf.drop(list(df_uf.filter(regex=\"s__\")),axis=1,inplace=False) #Drops columns that include features at species level\n",
    "    df_uf = df_uf.filter(regex=\"g__\") #Drops columns that include features broader than genus level\n",
    "    return df_uf"
   ],
   "id": "4bcf513db7515b8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_genus = filter_genus(X_train)\n",
    "X_test_genus = filter_genus(X_test)"
   ],
   "id": "866229db722bbf2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\"target_names\" in X_train_genus.columns",
   "id": "4094ea110df59e74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Features used by model:\")\n",
    "print(X_train_genus.columns.tolist())"
   ],
   "id": "ad098b321d34960d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Random Forest Regressor with Train/Test split (Genus)",
   "id": "7d59a7755b1444f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Base model",
   "id": "a9131d8365d705cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create model\n",
    "# Base model\n",
    "rf = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")"
   ],
   "id": "eda7d7dc405462a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Search for the best model",
   "id": "f2b359f109ab90c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800, 1000],\n",
    "    \"max_depth\": [None, 10, 20, 40],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "# Randomized search with 5-fold CV on training set\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit search\n",
    "search.fit(X_train_genus, Y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = search.best_estimator_\n",
    "print(\"\\nBest hyperparameters:\", search.best_params_)"
   ],
   "id": "8661bb165827c72c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# evaluate model\n",
    "yhat = best_model.predict(X_test_genus)\n",
    "mse = mean_squared_error(Y_test, yhat)\n",
    "print('Mean Squared Error: %.3f' % mse)\n",
    "\n",
    "# CV RMSE of best model\n",
    "best_cv_rmse = (-search.best_score_) ** 0.5\n",
    "print(\"Best CV RMSE:\", best_cv_rmse)\n",
    "\n",
    "#R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_test, yhat)\n",
    "print('R2 Score: %.3f' % r2)"
   ],
   "id": "f9364a0806c336f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=Y_test, y=yhat, alpha=0.6)\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs Actual (Genus-level RF)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ad574d564c7852a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "residuals = Y_test - yhat\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=yhat, y=residuals, alpha=0.6)\n",
    "plt.axhline(0, linestyle=\"--\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predictions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8da57ad565f614b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importances = pd.Series(\n",
    "    best_model.feature_importances_,\n",
    "    index=X_train_genus.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    x=importances.head(top_n),\n",
    "    y=importances.head(top_n).index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Genus\")\n",
    "plt.title(f\"Top {top_n} most important genera\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "6e8a5f262b8fc84d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "st = SuperTree(\n",
    "    best_model,\n",
    "    X_train_genus,\n",
    "    Y_train\n",
    ")\n",
    "\n",
    "st.show_tree(which_tree=0)"
   ],
   "id": "b052a6d63b392ba6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get predictions from each tree on the test set\n",
    "all_tree_preds = np.array([tree.predict(X_test_genus) for tree in best_model.estimators_])\n",
    "\n",
    "# compute the mean prediction (Random Forest final prediction)\n",
    "rf_pred = all_tree_preds.mean(axis=0)\n",
    "\n",
    "# compute standard deviation per sample (uncertainty)\n",
    "rf_std = all_tree_preds.std(axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# plot all tree predictions (semi-transparent lines)\n",
    "for i in range(all_tree_preds.shape[0]):\n",
    "    plt.plot(Y_test.values, all_tree_preds[i], 'o', color='lightgray', alpha=0.3)\n",
    "\n",
    "# plot Random Forest mean prediction\n",
    "plt.scatter(Y_test, rf_pred, color='blue', label='RF mean prediction', s=40)\n",
    "\n",
    "plt.errorbar(Y_test, rf_pred, yerr=rf_std, fmt='o', color='red', alpha=0.5, label='¬±1 std across trees')\n",
    "\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color='black', linestyle='--', label='Perfect prediction')\n",
    "\n",
    "plt.xlabel(\"Actual Age Group\")\n",
    "plt.ylabel(\"Predicted Age Group\")\n",
    "plt.title(\"Random Forest ‚Äì Forest plot of tree predictions\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "970874b827cb2a7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Alternative Models",
   "id": "c0793638701d012a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### XGBoost Alternative",
   "id": "89d196cc9e1d8306"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Base model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "#Best hyperparameters: {'subsample': 0.7, 'reg_lambda': 1.0, 'reg_alpha': 0.1, 'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.01, 'colsample_bytree': 0.2}"
   ],
   "id": "5ac4ab60ebe633dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Feature Selection via neural networks",
   "id": "2c98c1eec8ab3f95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The finding",
   "id": "e5a976626863a1fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, constraints, callbacks, initializers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# 1. LOCK DOWN RANDOMNESS (Reproducibility)\n",
    "# ---------------------------------------------------------\n",
    "def set_seeds(seed=42):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    print(f\"‚úÖ Random Seeds fixed to {seed}\")\n",
    "\n",
    "set_seeds(42)\n",
    "\n",
    "# 2. SETUP DATA\n",
    "# ---------------------------------------------------------\n",
    "X_train_tf = X_train.astype('float32')\n",
    "y_train_tf = Y_train.values.astype('float32')\n",
    "\n",
    "# 3. THE GATEKEEPER LAYER\n",
    "# ---------------------------------------------------------\n",
    "class GatekeeperLayer(layers.Layer):\n",
    "    def __init__(self, num_features, l1_penalty=0.01, **kwargs):\n",
    "        super(GatekeeperLayer, self).__init__(**kwargs)\n",
    "        self.num_features = num_features\n",
    "        self.l1_penalty = l1_penalty\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(self.num_features,),\n",
    "            # Start tiny (but not dead zero) so gradients can flow if needed\n",
    "            initializer=initializers.RandomUniform(minval=0.0, maxval=0.002),\n",
    "            trainable=True,\n",
    "            constraint=constraints.NonNeg(),\n",
    "            regularizer=regularizers.l1(self.l1_penalty)\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs * self.w\n",
    "\n",
    "# 4. BUILD & TRAIN FUNCTION (The \"Sweet Spot\" 32-Neuron Brain)\n",
    "# ---------------------------------------------------------\n",
    "def train_candidate(l1_strength, run_seed):\n",
    "    # Set seed again for this specific run to ensure diversity if we change seeds\n",
    "    tf.random.set_seed(run_seed)\n",
    "\n",
    "    inputs = layers.Input(shape=(X_train_tf.shape[1],))\n",
    "\n",
    "    # Layer 1: Selection\n",
    "    x = GatekeeperLayer(X_train_tf.shape[1], l1_penalty=l1_strength)(inputs)\n",
    "\n",
    "    # Layer 2: The \"Brain\" - Set to 32 as requested\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "                  loss='mse', metrics=['mae'])\n",
    "\n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train_tf, y_train_tf,\n",
    "        epochs=120,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        verbose=0,\n",
    "        callbacks=[callbacks.EarlyStopping(patience=8, restore_best_weights=True)]\n",
    "    )\n",
    "\n",
    "    best_mse = min(history.history['val_loss'])\n",
    "    val_rmse = best_mse ** 0.5\n",
    "\n",
    "    weights = model.layers[1].get_weights()[0]\n",
    "    n_selected = np.sum(weights > 1e-5)\n",
    "\n",
    "    return n_selected, val_rmse, weights\n",
    "\n",
    "# 5. THE MONTE CARLO SEARCH LOOP\n",
    "# ---------------------------------------------------------\n",
    "# Smoother gradient of penalties\n",
    "penalties = [0.5, 1.0, 2.0, 3.0, 5.0, 7.5, 10.0]\n",
    "repeats_per_penalty = 5\n",
    "\n",
    "champion_model = {\n",
    "    'rmse': float('inf'),\n",
    "    'n_features': 0,\n",
    "    'penalty': 0,\n",
    "    'weights': None\n",
    "}\n",
    "\n",
    "print(f\"{'Penalty':<8} | {'Run':<4} | {'Features':<10} | {'Val RMSE':<10} | {'Status'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for p in penalties:\n",
    "    for i in range(repeats_per_penalty):\n",
    "        # We vary the seed slightly per run so \"Repeats\" aren't identical\n",
    "        run_seed = 42 + i\n",
    "\n",
    "        n_feats, val_rmse, weights = train_candidate(l1_strength=p, run_seed=run_seed)\n",
    "\n",
    "        # Criteria: Between 50 and 1500 features\n",
    "        is_valid = n_feats < 1250 and n_feats > 50\n",
    "        status = \"‚ùå Too Many\"\n",
    "\n",
    "        if is_valid:\n",
    "            status = \"‚úÖ Candidate\"\n",
    "            if val_rmse < champion_model['rmse']:\n",
    "                champion_model['rmse'] = val_rmse\n",
    "                champion_model['n_features'] = n_feats\n",
    "                champion_model['penalty'] = p\n",
    "                champion_model['weights'] = weights\n",
    "                status = \"üèÜ NEW BEST\"\n",
    "        elif n_feats <= 50:\n",
    "            status = \"‚ö†Ô∏è Too Sparse\"\n",
    "\n",
    "        print(f\"{p:<8} | {i+1:<4} | {n_feats:<10} | {val_rmse:<10.2f} | {status}\")\n",
    "\n",
    "# 6. SAVE & EXPORT\n",
    "# ---------------------------------------------------------\n",
    "if champion_model['weights'] is not None:\n",
    "    print(\"\\n------------------------------------------------\")\n",
    "    print(f\"üéâ FINAL CHAMPION FOUND!\")\n",
    "    print(f\"Penalty: {champion_model['penalty']}\")\n",
    "    print(f\"Features: {champion_model['n_features']}\")\n",
    "    print(f\"Validation RMSE: {champion_model['rmse']:.4f}\")\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "    gate_weights = champion_model['weights']\n",
    "    df_imp = pd.DataFrame({'Bacteria': X_train.columns, 'Score': gate_weights})\n",
    "    selected_bacteria = df_imp[df_imp['Score'] > 1e-5].sort_values('Score', ascending=False)\n",
    "\n",
    "    print(\"Top 10 Selected Bacteria:\")\n",
    "    print(selected_bacteria.head(10))\n",
    "\n",
    "    X_train_elite = X_train[selected_bacteria['Bacteria']]\n",
    "    X_test_elite = X_test[selected_bacteria['Bacteria']]\n",
    "\n",
    "    print(f\"\\nCreated X_train_elite with shape: {X_train_elite.shape}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No models found in the sweet spot. Penalties might still be too low/high.\")"
   ],
   "id": "3ab54511c1ce4904",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Comparing the genus vs neural network features",
   "id": "199b9283e701d8f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import RepeatedKFold, cross_validate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 1. SETUP REPEATED VALIDATION\n",
    "# 7-fold, repeated 3 times = 21 total trainings per model\n",
    "# This is the \"Gold Standard\" for stability.\n",
    "rkf = RepeatedKFold(n_splits=7, n_repeats=3, random_state=42)\n",
    "\n",
    "datasets = {\n",
    "    \"Biology (Genus 1200)\": X_train_genus,\n",
    "    \"Math (NN Elite)\": X_train_elite\n",
    "}\n",
    "\n",
    "# 2. DEFINING THE CHAMPIONS\n",
    "# Using your optimized hyperparameters\n",
    "models = {\n",
    "    \"XGBoost\": xgb.XGBRegressor(\n",
    "        n_estimators=1000, learning_rate=0.01, max_depth=3,\n",
    "        subsample=0.7, colsample_bytree=0.2, reg_alpha=0.1,\n",
    "        reg_lambda=1.0, n_jobs=-1, random_state=42\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=1000, max_depth=20, max_features='sqrt',\n",
    "        n_jobs=-1, random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "# 3. THE ULTIMATE BATTLE\n",
    "results = []\n",
    "\n",
    "print(f\"{'Dataset':<20} | {'Model':<15} | {'Avg R2':<10} | {'Std Dev':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for data_name, X_data in datasets.items():\n",
    "\n",
    "    # Clean column names\n",
    "    X_clean = X_data.copy()\n",
    "    X_clean.columns = [re.sub('[^A-Za-z0-9_]+', '', str(col)) for col in X_clean.columns]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Run 21-fold CV\n",
    "        cv_results = cross_validate(\n",
    "            model, X_clean, Y_train,\n",
    "            cv=rkf,\n",
    "            scoring='r2',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        mean_r2 = np.mean(cv_results['test_score'])\n",
    "        std_r2 = np.std(cv_results['test_score'])\n",
    "\n",
    "        print(f\"{data_name:<20} | {model_name:<15} | {mean_r2:.4f}     | ¬±{std_r2:.3f}\")\n",
    "\n",
    "        results.append({\n",
    "            'Dataset': data_name,\n",
    "            'Model': model_name,\n",
    "            'R2': mean_r2,\n",
    "            'Std': std_r2\n",
    "        })\n",
    "\n",
    "print(\"-\" * 65)"
   ],
   "id": "802c3b1fc04a2922",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
